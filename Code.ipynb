{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-ynMUYHx6Pn"
   },
   "source": [
    "# CA3 - Bayesian Network Text Classification\n",
    "The project's aim is to catagorize the news into three different genres. To do so, **bayesian networks** are being applied in order to consider every word to contribute independently to the probability irrespective of the correlations. In this way, the computations will be lessened as well as having a reasonable final result.\n",
    "## Libraries\n",
    "> `time` library is being used to measure the time needed to reach the final network.\n",
    "<br>\n",
    "`operator` library is being used in the process of finding the final labels for test data.\n",
    "<br>\n",
    "As it is known, `numpy` and `pandas` libraries are two key factors in processing data. With using these tools, working with texts would be much more easier.\n",
    "<br>\n",
    "`nltk` plays a crucial role in this project. As it is needed to clean the input data, this library is a great choice to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k42K26H8x6Po"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from IPython.display import Markdown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0_PZp_gx6Pr"
   },
   "source": [
    "## Input Data\n",
    ">Two `.csv` files are given as the input data if this problem.\n",
    "<br><br>\n",
    "The first one is *data.csv* which is the the source file of this problem. To create a bayesian network, an input file is needed to extract the words to make a dictionary consisting of all the words which have been used before and their frequency as well. This input file contains many columns in which **short_description** and **header** were the useful ones as they have the most frequent words related to that specific subject. Additionally, **category** column is needed to keep the words in their own genres. Other columns of this input file are useless in this project.\n",
    "<br><br>\n",
    "The second file is *test.csv* which contain the same information as the first file, the only difference is that this file does not support any kind of data related to **category** of these news. The aim is to use the dictionary which have been built using the first file, and find the category of each news in the test file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-U-0z8vx6Ps"
   },
   "source": [
    "## Partitioning input data\n",
    ">In classification tasks, it is important to test the final network to see whether it is working well or not. To do so, test data is needed which the system has not seen before. partitioning the data, hence, will do the same. According to this problem's data, separating 80 percent of the input for *training* and 20 percent for *validation* would be a reasonable choice. \n",
    "<br>\n",
    "Note that to do so, we need to subtract 20 percent of each category for validation, and leave others as train data. If we just cut 20 percent of data and not from each category, the number of samples in train and test data may be unfair. For example, we may put most of a specific category in test data. In this way, the network will suffer from not having enough train data in that selected category.\n",
    "\n",
    "## OverSampling\n",
    ">Imbalanced learning is a result of imbalanced data. It means that the number of samples for each category is not equal to any other one. So this will lead the system to a point that many differences can be seen in Recall and Precision of different groups. \n",
    "<br>    \n",
    "In order to tackle this issue, **oversampling** concept can be used. This concept suggest that for the catagories with less data, we can copy and repeat the samples we had before to make a dataset with equal number of examples in each category. In this way the imbalanced problem will be lessend, but *overfitting* may happen as the network is fitting to the input data more than its usual form. So, using this feature is kind of a trade-off and have to be controlled. \n",
    "<br><br>\n",
    "In this problem, as many approaches has been applied before using oversampling, this feature will not add a significant amount of accuracy, recall, or precision to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeBWJyamx6Ps"
   },
   "outputs": [],
   "source": [
    "def partition_data(data):\n",
    "\n",
    "    data = data.sample(frac=1)\n",
    "    grouped_data = data.groupby([\"category\"])\n",
    "    train_prerequisits = []\n",
    "    test_prerequisits = []\n",
    "    max_size = data['category'].value_counts().max()\n",
    "    \n",
    "    for index, value in enumerate(data[\"category\"].unique()):\n",
    "        selected_group = grouped_data.get_group(value)\n",
    "        partition = int(0.8 * len(selected_group))\n",
    "        train_prerequisits.append(selected_group[0:partition])\n",
    "        test_prerequisits.append(selected_group[partition: ])\n",
    "\n",
    "    train_prerequisits_oversampled = oversample(train_prerequisits, max_size)\n",
    "    train_data = pd.concat([train_prerequisits_oversampled[i] for i in range(len(train_prerequisits_oversampled))])\n",
    "\n",
    "    test_data = pd.concat([test_prerequisits[i] for i in range(len(test_prerequisits))])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def oversample(data, max_size):\n",
    "    for index, value in enumerate(data):\n",
    "        value.append(value.sample(max_size - len(value), replace=True), ignore_index = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv66ImBfx6Pv"
   },
   "source": [
    "## Bayesian Rules\n",
    "\n",
    "> This approach tries to classify each category using the formula explained below:\n",
    "> $$ P(category\\ |\\ word) = \\frac{P(word\\ |\\ category)P(category)}{P(word)} $$\n",
    ">\n",
    ">\n",
    "> #### $P(category\\ |\\ word)$\n",
    ">> As it was explained in before, we are looking for building a bayesin classifier. In order to do so, we need to have the probability of each category for every instance. In this way, the one with the biggest probability, will be the detected category. So, the key to this problem is to find $P(category\\ |\\ sample)$ which is being obtained using $P(category\\ |\\ word)$ for each word in sample. This probability is Posterior Probability.\n",
    ">>\n",
    ">\n",
    "> #### $P(word\\ |\\ category)$\n",
    ">> According to bayes rules, as the posterior probability is not computable, we can obtain its probability using \n",
    "$P(word\\ |\\ category)$. This probability, called Likelihood, represents the probbility of having each word in a specific category. To calculate this parameter, a dictionary is being created in which every word and its frequency in all the categories have been calculated before.\n",
    ">>\n",
    ">\n",
    "> #### $P(category)$ \n",
    ">> This refers to the probability of the selected category. Each category has a chance equal to $\\frac{number\\ of\\ samples\\ in\\ category\\ x}{number\\ of\\ all\\ samples} $. This probability is called Class Prior Probability.\n",
    ">>\n",
    ">\n",
    "> #### $P(word)$\n",
    ">> This parameter, known as Predictor Prior Probability, represents the total probability of having a word in train data. Since it is similar in all cases that are being examined, this part is usually being ommited.\n",
    ">>\n",
    "> \n",
    "\n",
    "## The Solver\n",
    "To create a network, a python class is needed to process train data and create a dictionary in which we keep the words and their probability for each different category. \n",
    "<br>\n",
    "> #### 1. `prepare_dictionaries()`\n",
    "As this problem is being solved using a bayesian network, a dictionary for each category is needed in which the words and their frequency is being recorded. In order to do, first we initialize a dictionary for each category and then, create a key for each word that is being seen in that specific category. The value of each key refers to its frequency in the selected category. Using this 2D dictionary(one dimension for different categories and the other for words) the probability of each word is calculable.\n",
    "<br>\n",
    "It should be noted that in this stage, a set of all the words in all the descriptions will be created as well. This set will be used in further computations.\n",
    "> #### 2. `convert_description_to_words()`\n",
    "In order to obtain enough information from input data, it is cruical to tokenize and clean the input data. This function has the aim of doing so. <br>\n",
    "Firstly, a tokenizer will be initialized using `RegexTokenizer` function. After extract the words, it is needed to do a preprocess on them. In this part, we will eliminate stop words which are repetitious and are being used in almost every sentence by comparing words with a set of stopwords obtained from `nltk` library. Next, all the capital alphabets should be converted to small characters in order to be equal with the same word but different in capital alphabets. Finally, it is important to find the origin of each word. For example, *starts* and *start* are not different from each other. To prevent this from happening, a tool called **lemmatizer** from `nltk` library is being used. This will convert each word to its origin and will help us to have a same key for all of similar words. The list of words will be returned after processing them in this way.\n",
    "> #### 3. `num_of_words()`\n",
    "This function simply counts the number of words in a selected category. The result will be needed in predicting labels further.\n",
    "> #### 4. `find_labels()`\n",
    "This function is being used in order to predict the labels of a group of test data. To do so, **Bayesian Rules** should be applied. \n",
    "<br> <br>\n",
    "$$P(category\\ |\\ description) = \\frac{P(description\\ |\\ category)P(category)}{P(description)} $$\n",
    "<br>\n",
    "$$P(description\\ |\\ category) = \\prod\\limits_{for\\ word\\ in\\ ala\\ words\\ of\\ a\\ description} P(word\\ |\\ category) $$\n",
    "According to bayes rule, we can calculate the probability of each category using these formulas. This function is computing the probability of each category for each data and chooses the one which has the biggest value as the predicted category. <br>\n",
    "Note that in each step of computation, it is better to multiply the probabilities with a bug number in order to prevent from a number to lead to 0.\n",
    "> #### 5. `apply_on_test()`\n",
    "This function simply tries to predict the labels of the *test.csv* file. First, the test file will be read and then, applied `find_labels()` function on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NshhiTlyx6Pv"
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, train_data):\n",
    "        self.train_data = train_data.reset_index(drop=True)\n",
    "        self.words, self.dictionaries = self.prepare_dictionaries()\n",
    "    \n",
    "    def prepare_dictionaries(self):\n",
    "        all_words = set()\n",
    "        train_dict = dict()\n",
    "        \n",
    "        for index, value in enumerate(self.train_data[\"category\"].unique()):\n",
    "            train_dict[value] = dict()\n",
    "        \n",
    "        for index, value in enumerate(self.train_data['short_description']):\n",
    "            list_of_words = self.convert_description_to_words(value)\n",
    "            for i, word in enumerate(list_of_words):\n",
    "                all_words.add(word)\n",
    "                selected_catagory = self.train_data[\"category\"][index]\n",
    "                if word in train_dict[selected_catagory]:\n",
    "                    train_dict[selected_catagory][word] += 1\n",
    "                else:\n",
    "                    train_dict[selected_catagory][word] = 1\n",
    "\n",
    "        return all_words, train_dict\n",
    "    \n",
    "    def convert_description_to_words(self, value):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        final_words=[]\n",
    "        list_of_words = tokenizer.tokenize(value)\n",
    "\n",
    "        for i, value in enumerate(list_of_words):\n",
    "            if value in stop_words:\n",
    "                continue\n",
    "            word = value.lower()\n",
    "            final_words.append(lemmatizer.lemmatize(word))\n",
    "            \n",
    "        return final_words\n",
    "    \n",
    "    def num_of_words(self, category):\n",
    "        ans = 0\n",
    "        for index, (key, value) in enumerate(self.dictionaries[category].items()):\n",
    "            ans += value\n",
    "        return ans\n",
    "    \n",
    "    def find_labels(self, data):\n",
    "        labels = []\n",
    "        prior = dict()\n",
    "        num_of_words = dict()\n",
    "        grouped_data = self.train_data.groupby([\"category\"])\n",
    "\n",
    "        \n",
    "        for index, value in enumerate(self.train_data[\"category\"].unique()):\n",
    "            num_of_words[value] = self.num_of_words(value)\n",
    "            prior[value] = len(grouped_data.get_group(value)) / len(self.train_data)\n",
    "\n",
    "        for index, value in enumerate(data['short_description']):\n",
    "            p = dict()\n",
    "            all_words = self.convert_description_to_words(value)\n",
    "            \n",
    "            for i, category in enumerate(self.train_data[\"category\"].unique()):\n",
    "                p[category] = prior[category]\n",
    "                \n",
    "                for index, word in enumerate(all_words):\n",
    "                    if word in self.dictionaries[category]:\n",
    "                        p[category] *= (self.dictionaries[category][word] + 1)\n",
    "                    p[category] *= 20000 / (len(self.words) + num_of_words[category])\n",
    "\n",
    "            labels.append(max(p.items(), key=operator.itemgetter(1))[0])\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def apply_on_test(self, test_file):\n",
    "        \n",
    "        test_data = pd.read_csv(test_file)\n",
    "        test_data = test_data.replace(np.nan, '', regex=True)\n",
    "        test_data['short_description'] = test_data['short_description'] + \" \" + test_data['headline']\n",
    "        test_data = pd.DataFrame(test_data['short_description'])\n",
    "\n",
    "        return self.find_labels(test_data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgHR5vNgx6Py"
   },
   "source": [
    "## Precision, Recall, Accuracy, and Confusion Matrix\n",
    "As it was said in the project, It is cruical to ensure that this network is precise enough. To measure this, some standards have been defined.\n",
    "> **1. Confusion Matrix**\n",
    "> A confusion matrix is a way of showing prediction results briefly on a classification problem(in this case Bayesian Network). The number of correct and incorrect label predictions are summarized with count values and broken into each class. In this way, we can see for each label how many predictions were correct and what was the type of our mistakes and the number of them as well. From observing Confusion Matix, accuracy, precision and recall can be calculated easily as this matrix have the summery of all the information needed to examine a classification method. <br><br>\n",
    "> **2. Accuracy**\n",
    "> Accuracy is the ratio of the total number of correct detections divide to total number of predictions. This measure examines the system generally.<br>\n",
    "$$Accuracy = \\frac{Number\\ of\\ correct\\ detections}{Number\\ of\\ all\\ predictions}$$\n",
    "> **3. Precision**\n",
    "> Precision can be defined as the ratio of the total number of correctly classified the specified label divide to total number of predictions with that specific label. In fact, this measure shows that how much predicting a specific label is accurate. High precision indicates the probability which an examples' label is(if this category has been selected). <br><br>\n",
    "$$Precision = \\frac{Number\\ of\\ correct\\ detections\\ for\\ a\\ specific\\ category}{Number\\ of\\ all\\ predictions\\ with\\ this\\ specific\\ category}$$\n",
    "<br><br>\n",
    "> **4. Recall**\n",
    "> Recall can be defined as the ratio of the total number of correctly classified the specified label divide to the total number of examples with that selected label. Actually, high recall rate shows that the category is being recognized right in most of the cases. <br>\n",
    "$$Recall = \\frac{Number\\ of\\ correct\\ detections\\ for\\ a\\ specific\\ category}{Number\\ of\\ all\\ examples\\ with\\ this\\ specific\\ category}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUL4OAgx6Py"
   },
   "outputs": [],
   "source": [
    "def precision(index, confusion_matrix):\n",
    "    col = confusion_matrix[:, index]\n",
    "    return confusion_matrix[index, index] / col.sum()\n",
    "    \n",
    "def recall(index, confusion_matrix):\n",
    "    row = confusion_matrix[index, :]\n",
    "    return confusion_matrix[index, index] / row.sum()\n",
    "\n",
    "def accuracy(confusion_matrix):\n",
    "    return confusion_matrix.trace() / confusion_matrix.sum() \n",
    "\n",
    "def calc_confusion_matrix(actual, predicted):\n",
    "    labels = pd.DataFrame(list(zip(actual, predicted)), columns =['Actual', 'Predicted'])\n",
    "    size = len(labels['Actual'].unique())\n",
    "    confusion_matrix = np.zeros((size, size))\n",
    "    \n",
    "    dict_label = dict()\n",
    "    for index, value in enumerate(labels['Actual'].unique()):\n",
    "        dict_label[value] = index\n",
    "    \n",
    "    for i in range(len(labels['Actual'])):\n",
    "        confusion_matrix[dict_label[labels['Actual'][i]]][dict_label[labels['Predicted'][i]]] += 1\n",
    "        \n",
    "    return confusion_matrix, dict_label\n",
    "\n",
    "def print_result(category, index, confusion_matrix):\n",
    "    display(Markdown(\"### The result for \" + category.lower() + \" category is shown below:\\n\"))\n",
    "    display(\"Percision: %s\" % precision(index, confusion_matrix))\n",
    "    display(\"Recall: %s\" % recall(index, confusion_matrix))\n",
    "    \n",
    "def print_confusion_matrix(dict_label, confusion_matrix):\n",
    "    table = \"|Category|\"\n",
    "    for i, (key, value) in enumerate(dict_label.items()):\n",
    "        table += key\n",
    "        table += \"|\"\n",
    "    table += \"\\n|---\"\n",
    "    for i in range(len(confusion_matrix[0])):\n",
    "        table += \"|---\"\n",
    "    table += \"|\\n\"\n",
    "    for i, (key1, value1) in enumerate(dict_label.items()):\n",
    "        table += \"|\"\n",
    "        table += key1\n",
    "        table += \"|\"\n",
    "        for i, (key2, value2) in enumerate(dict_label.items()):\n",
    "            table += (confusion_matrix[value1][value2]).astype('str')\n",
    "            table += \"|\"\n",
    "        table += \"\\n\"\n",
    "    table += \"<caption  style=\\\"text-align:center\\\">Confusion Matrix</caption>\" \n",
    "    display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf1Uq6xHx6P0"
   },
   "source": [
    "## Initial steps to clean input data\n",
    "> Firstly, the input data will be converted to a pandas data structure using `read_csv()` function from `pandas` library. \n",
    "<br>\n",
    "One of the first problems which is being faced is that some of columns have **Nan** value. To prevent errors, all the **Nan** values will be replaced with a space(' ' character) `replace()` function. After all, We need to keep the data in **short_description**, **header** and **category** columns and the rest will not be used. To do so, two measures has to be taken. \n",
    "<br>\n",
    "Firstly, as this method works with bag of words and does not care about the sentences, we can combine **header** and **short_description** columns into a single column. All the data needed to create a dictionary will be kept in **short_description** column. Lastly, the needed data will consist of the new **short_description** column and **category** column, and the rest of data will be thrown away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOU3j9ipx6P1"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Attachment/data.csv\")\n",
    "data = data.replace(np.nan, '', regex=True)\n",
    "data['short_description'] = data['short_description'] + \" \" + data['headline']\n",
    "data = pd.DataFrame(data = {'category' : data.loc[:, 'category'], 'short_description' : data.loc[:, 'short_description']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEcKfIbXx6P3"
   },
   "source": [
    "## Phase One\n",
    "> In this part of the problem, we are about to predict labels for the input data. The aim is to check the network on only two different categories. In order to do so, data is being categorized using `groupby()` and then, by applying `get_group()` function, the two proposed groups are being extracted.\n",
    "<br>\n",
    "After all, by creating an instance of the solver class which has been declared before in the report, and the function `find_labels()`, the labels will be predicted. Finally, by the functions which has been defined to find the effectiveness of the network, the total accuracy, recall and precision will be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArmJSIiax6P3"
   },
   "outputs": [],
   "source": [
    "grouped_data = data.groupby([\"category\"])\n",
    "phase_one_data = pd.concat([grouped_data.get_group('TRAVEL'), grouped_data.get_group('BUSINESS')])\n",
    "train_data_phase_one, test_data_phase_one = partition_data(phase_one_data)\n",
    "solve_phase_one = Solver(train_data_phase_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzSZ79Tox6P7",
    "outputId": "0bbc7ebb-7f00-4a81-dbf8-1443495fd2cd"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### The confusion matrix is shown below: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Category|TRAVEL|BUSINESS|\n",
       "|---|---|---|\n",
       "|TRAVEL|1716.0|64.0|\n",
       "|BUSINESS|90.0|979.0|\n",
       "<caption  style=\"text-align:center\">Confusion Matrix</caption>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Total accuracy is: 0.9459459459459459"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The result for travel category is shown below:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Percision: 0.9501661129568106'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall: 0.9640449438202248'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The result for business category is shown below:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Percision: 0.9386385426653883'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall: 0.9158091674462114'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = solve_phase_one.find_labels(test_data_phase_one)\n",
    "confusion_matrix, dict_label = calc_confusion_matrix(test_data_phase_one['category'], prediction)\n",
    "display(Markdown(\"### The confusion matrix is shown below: \"))\n",
    "print_confusion_matrix(dict_label, confusion_matrix)\n",
    "\n",
    "display(Markdown(\"### Total accuracy is: \" + str(accuracy(confusion_matrix))))\n",
    "for index, value in enumerate(test_data_phase_one['category'].unique()):\n",
    "    print_result(value, dict_label[value], confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpvrP7M0x6P-"
   },
   "source": [
    "| Phase One | Travel | Business | \n",
    "| --- | --- | --- | \n",
    "| Recall | 0.9640 | 0.9158 | \n",
    "| Precision | 0.9502 | 0.9386 | \n",
    "| Accuracy(Total) | | 0.9459 | \n",
    "<caption  style=\"text-align:center\">Result for testing phase one of the project</caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRR2iBomx6P-"
   },
   "source": [
    "## Phase Two\n",
    "> In this part of the problem, we are about to predict labels for the input data. The difference is that we want to check the network on all different categories in this stage. In order to do so, data is being categorized using `groupby()` and then, by applying `get_group()` function, the two proposed groups are being extracted.\n",
    "<br>\n",
    "After all, by creating an instance of the solver class which has been declared before in the report, and the function `find_labels()`, the labels will be predicted. Finally, by the functions which has been defined to find the effectiveness of the network, the total accuracy, recall and precision will be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PtIKiwGx6P_"
   },
   "outputs": [],
   "source": [
    "train_data_phase_two, test_data_phase_two = partition_data(data)\n",
    "solve_phase_two = Solver(train_data_phase_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rhD9U1Nx6QB",
    "outputId": "6d2055d5-1b63-4b44-eff6-9edb83604e27"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### The confusion matrix is shown below: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Category|BUSINESS|STYLE & BEAUTY|TRAVEL|\n",
       "|---|---|---|---|\n",
       "|BUSINESS|941.0|26.0|102.0|\n",
       "|STYLE & BEAUTY|70.0|1598.0|69.0|\n",
       "|TRAVEL|67.0|31.0|1682.0|\n",
       "<caption  style=\"text-align:center\">Confusion Matrix</caption>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Total accuracy is: 0.920409943305713"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The result for business category is shown below:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Percision: 0.87291280148423'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall: 0.8802619270346118'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The result for style & beauty category is shown below:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Percision: 0.965558912386707'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall: 0.9199769717904432'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### The result for travel category is shown below:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Percision: 0.9077172153264975'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall: 0.9449438202247191'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = solve_phase_two.find_labels(test_data_phase_two)\n",
    "confusion_matrix, dict_label = calc_confusion_matrix(test_data_phase_two['category'], prediction)\n",
    "display(Markdown(\"### The confusion matrix is shown below: \"))\n",
    "print_confusion_matrix(dict_label, confusion_matrix)\n",
    "\n",
    "display(Markdown(\"### Total accuracy is: \" + str(accuracy(confusion_matrix))))\n",
    "for index, value in enumerate(test_data_phase_two['category'].unique()):\n",
    "    print_result(value, dict_label[value], confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv5cNZ6Lx6QD"
   },
   "source": [
    "| Phase Two | Travel | Business | Style & Beauty |\n",
    "| --- | --- | --- | --- |\n",
    "| Recall | 0.9449 | 0.8802 | 0.9199 |\n",
    "| Precision | 0.9077 | 0.8729 | 0.9656 |\n",
    "| Accuracy(Total) | | 0.9204 |\n",
    "<caption  style=\"text-align:center\">Result for testing phase two of the project</caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ONlT2khx6QE"
   },
   "source": [
    "## Test the `test.csv` file\n",
    "Finally, the network should be applied on a test data which does not have specific categories. To do so, `apply_on_test()` function from solver class should be used. This function has been declared and explained before in this project. After finding the labels, they are being saved using `to_csv()` function and create a *csv* flie named *output.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSNmy80Xx6QF",
    "outputId": "b2218d8b-9181-4440-a4fe-fd5424a2ef8d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It takes 2.5437231063842773 seconds to predict the labels of test data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ans = solve_phase_two.apply_on_test(\"Attachment/test.csv\")\n",
    "answer = pd.DataFrame(list(zip([i for i in range(len(ans))], ans)), columns =['index', 'category'])\n",
    "answer.to_csv ('output.csv', index = False, header=True)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "display(Markdown(\"It takes %s seconds to predict the labels of test data.\" %total_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_MUkKjox6QH"
   },
   "source": [
    "# Questions and Explanations\n",
    "\n",
    "**1. Stemming or Lemmatization**\n",
    "> The application of both stemming and lemmatization is to produce morphological variants of a root/base word with the aim of reducing inflectional forms and sometimes derivationally related forms.\n",
    "> **stemming:** Stems are created by removing the suffixes or prefixes used with a word. Stemming a word or sentence may result in creating words that dows not really exist. For example, the word *troubling* will change into *troubl* and not the right form(*trouble*) <br>\n",
    "> **Lemmatization:** Lemmatization, unlike Stemming, reduces the inflected words while ensuring that the base word belongs to the language.\n",
    "<br>\n",
    ">> According to the definitions of these two approaches, obviously it is better to use Lemmatization inasmuch as it reachs the real origin of each word. As an instance, the word *'studies'* will be *'studi'* using Stemming but if Lemmatization is being pplied, the word will change into its source, *'study'*.\n",
    "\n",
    "**2. TF-IDF**\n",
    "> TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect the importance of each document in a collection. It is often used as a weighting factor in searches of information retrieval, text mining, and calssification as well.\n",
    "<br>\n",
    ">>**TF(Term Frequency):** TF is actually calculating the frequency of a word in a document. The thought behind it is that if a word occurs multiple times in a document, its relevance should be more as it should be more meaningful than other words that appear fewer times.\n",
    "<br>\n",
    "$$ TF(word,documanet) = \\frac{count\\ of\\ word\\ in\\ documanet}{number\\ of all\\ words\\ in\\ documanet} $$\n",
    "<br>\n",
    ">>**IDF(Inverse Document Frequency):** If a word occurs many times in a document and also along many other documents, it may be because of its general meaning; not because it was relevant or meaningful. In other words, IDF measures the rank of the specific word for its relevancy within the text and not its frequency. Stop words which contain unnecessary information such as “a”, “into” and “and” are a good example of less important words as they are being used generally.\n",
    "<br>\n",
    "$$ IDF(word) = \\frac{number\\ of\\ all\\ ducuments}{occurrence\\ of\\ word\\ in\\ documents+1} $$\n",
    ">>In the furmula, the denominator is incremented in order to prevent the problem of not having a word in all the documents at all.\n",
    ">><br>\n",
    "**Revelance:** All in all, the total TF-IDF weight for a token in a document is:\n",
    "$$ TF-IDF = TF \\times IDF $$\n",
    ">>\n",
    "> **How to use this measurement in Bayesian Networks?** <br><br>\n",
    ">As it was said before, the probability for each word being in a certain category is being calculated using ites frequency in the selected documents. \n",
    "<br>\n",
    "This can easily change into TF-IDF measure as this approach is returning a value based on the importance of words. After doing so, the probability will be calculated in the way shown below:\n",
    "<br>\n",
    "$$ P(word\\ |\\ category) = \\frac{TF-IDF\\ value\\ of\\ the\\ word\\ in\\ category\\ + 1}{Sum\\ of\\ all\\ the\\ TF-IDF\\ values\\ in\\ category\\ + number\\ of\\ all\\ words} $$\n",
    "\n",
    "**3. High Precision**\n",
    ">If we just pay attention to the precision and not recall, we won't recognize that how often the system even recognizes an example which its real category is the one we are looking for. Consider a system that diagnoses corona virus in people. If the system has low recall and high precision, if the result is true, most of the time the patient's test was positive as well, but the problem is that the system cannot recognize many of them due to its low recall. As a result there will be lots of patients who got a negative result even though they had the corona virus. Hence, there will be plenty of problems as a result of low recall in this system.\n",
    "\n",
    "**4. New words**\n",
    ">As we know, if a word is not in the dictionary of a category, the probability of that specific word will be 0 result in a probability of 0 for that category, which will not be true most of the times. To prevent this from happening, here's a solution:\n",
    "<br>\n",
    "$$ P(word\\ |\\ category) = \\frac{frequency\\ of\\ the\\ word\\ in\\ category\\ + 1}{number\\ of\\ words\\ in\\ category\\ + number\\ of\\ all\\ words} $$\n",
    ">>In this way, we are actually assuming that each word is being repeated at least one time in each category. Hence, this will give a little value to the probability that was detected as zero before, and will not ruin the whole probability realted to a specific category.\n",
    "<br><br>\n",
    ">>> - The Answer to the Question: <br> It has been obtained that if a word is repeated just in one category, without this method, the only category that has a value except zero is the one which has the word actually. In other words, the rest of the sentence will have no importance dur to the zero probability."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CA3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
